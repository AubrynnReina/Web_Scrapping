{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://animehay.in/phim-moi-cap-nhap/trang-1.html'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_items = soup.find_all('div', class_='movie-item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_checkpoint_data(movie) -> tuple[int, str]:\n",
    "\n",
    "    movie_id = int(movie['id'][9:])\n",
    "    movie_latest_ep = movie.find('div', class_='episode-latest').text\n",
    "    \n",
    "    return (movie_id, movie_latest_ep)\n",
    "\n",
    "\n",
    "def save_checkpoint(checkpoint_set):\n",
    "\n",
    "    import pickle\n",
    "    CHECKPOINT_PATH = './data/checkpoint.pkl'\n",
    "\n",
    "    with open(CHECKPOINT_PATH, 'wb') as f:\n",
    "        pickle.dump(checkpoint_set, f)\n",
    "\n",
    "\n",
    "def load_checkpoint() -> set[int, str]:\n",
    "    import pickle\n",
    "    CHECKPOINT_PATH = './data/checkpoint.pkl'\n",
    "\n",
    "    with open(CHECKPOINT_PATH, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_movie_items(page):\n",
    "\n",
    "    url = f'https://animehay.in/phim-moi-cap-nhap/trang-{page}.html'\n",
    "    html_page = requests.get(url)\n",
    "    soup = BeautifulSoup(html_page.content, 'html.parser')\n",
    "    movie_items = soup.find_all('div', class_='movie-item')\n",
    "\n",
    "    return movie_items\n",
    "\n",
    "\n",
    "\n",
    "def extract_data(movie) -> tuple:\n",
    "    \n",
    "    from requests import  ConnectionError\n",
    "\n",
    "    link = movie.select('a:nth-child(2)')[0]['href']\n",
    "    response = requests.get(link)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        return ()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    movie_data = soup.find('div', class_='last')\n",
    "\n",
    "    if movie_data is None:\n",
    "        return ()\n",
    "    \n",
    "    name = soup.find('h1', class_='heading_movie').text.strip()\n",
    "\n",
    "    raw_categories = movie_data.find('div', class_='list_cate') \\\n",
    "                            .text.split('\\n')[4:-1:2]\n",
    "    categories = [category.strip() for category in raw_categories]\n",
    "\n",
    "    movie_status = movie_data.find('div', class_='status') \\\n",
    "                    .select('div:nth-child(2)')[0] \\\n",
    "                    .text.strip()\n",
    "    \n",
    "    score_and_review = movie_data.find('div', class_='score') \\\n",
    "                                 .select('div:nth-child(2)')[0].text.split()[:-2:2]\n",
    "    score = float(score_and_review[0]) if score_and_review[0] != 'NaN' else None\n",
    "    review = int(score_and_review[1]) if score_and_review[1] != 'NaN' else None\n",
    "\n",
    "    publish_year_str = movie_data.find('div', class_='update_time') \\\n",
    "                                 .select('div:nth-child(2)')[0].text.strip()\n",
    "    publish_year = int(publish_year_str) if publish_year_str != 'NaN' else None\n",
    "    \n",
    "    duration = movie_data.find('div', class_='duration') \\\n",
    "                        .select('div:nth-child(2)')[0].text.strip()\n",
    "    \n",
    "    return (name, categories, movie_status, score, review, publish_year, duration, link)\n",
    "\n",
    "def convert_to_dict(id, data) -> dict:\n",
    "\n",
    "    result_dict = {\n",
    "        'id': id,\n",
    "        'name': data[0],\n",
    "        'genre': data[1],\n",
    "        'status': data[2],\n",
    "        'score': data[3],\n",
    "        'review': data[4],\n",
    "        'publish_year': data[5],\n",
    "        'duration': data[6],\n",
    "        'link': data[7]\n",
    "    }\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "def append_data(id, data, data_dict):\n",
    "    \n",
    "    for i in range(len(data[1])):\n",
    "\n",
    "        for j, key in enumerate(data_dict):\n",
    "\n",
    "            if j == 0:  \n",
    "                data_dict[key].append(id)\n",
    "            \n",
    "            elif j == 2:\n",
    "                data_dict[key].append(data[j - 1][i])\n",
    "                \n",
    "            else:\n",
    "                data_dict[key].append(data[j - 1])\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "\n",
    "    import json\n",
    "    \n",
    "    with open(f'./data/json/{filename}.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "\n",
    "    filename = datetime.today().strftime('%Y_%m_%d')\n",
    "    data_df = pd.DataFrame(data=data)\n",
    "    data_df = data_df.explode('genre')\n",
    "    data_df.to_csv(f'./data/csv/{filename}.csv', index=False)\n",
    "\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def save_to_parquet(data_df, filename):\n",
    "\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "\n",
    "    data_table = pa.Table.from_pandas(data_df)\n",
    "    pq.write_table(data_table, f'./data/parquet/{filename}.parquet')\n",
    "\n",
    "\n",
    "def save_data(data):\n",
    "\n",
    "    from datetime import datetime\n",
    "\n",
    "    filename = datetime.today().strftime('%Y_%m_%d')\n",
    "\n",
    "    save_to_json(data, filename)\n",
    "    data_df = save_to_csv(data, filename)\n",
    "    save_to_parquet(data_df, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "      id                                               name  \\\n",
      "0   3719                                    Đan Đạo Tông Sư   \n",
      "1   3776        Mobile Suit Gundam: Suisei no Majo Season 2   \n",
      "2   3781                                               Ở Rể   \n",
      "3   3727                                         Jigokuraku   \n",
      "4   3752                                             Mashle   \n",
      "5   3734  Isekai de Cheat Skill wo Te ni Shita Ore wa, G...   \n",
      "6   3775                               The Marginal Service   \n",
      "7   3771                                         Oshi no Ko   \n",
      "8   3770                                     World Dai Star   \n",
      "9   3766               Kanojo ga Koushaku-tei ni Itta Riyuu   \n",
      "10  3760                                   Kizuna no Allele   \n",
      "11  3733        Jijou wo Shiranai Tenkousei ga Guigui Kuru.   \n",
      "12  3761                    Mahou Shoujo Magical Destroyers   \n",
      "13  3762                             Megami no Café Terrace   \n",
      "14  3763                              Rokudou no Onna-tachi   \n",
      "15  3756                                   Otonari ni Ginga   \n",
      "16  3755                      Isekai Shoukan wa Nidome desu   \n",
      "17  3758                       Isekai One Turn Kill Nee-san   \n",
      "18  3729                 Yamada-kun to Lv999 no Koi wo Suru   \n",
      "19  3728                                  Tengoku Daimakyou   \n",
      "20  3759                                   Kawaisugi Crisis   \n",
      "21  3748                  Watashi no Yuri wa Oshigoto desu!   \n",
      "22  3753                                         Edomae Elf   \n",
      "23  3757            Birdie Wing: Golf Girls' Story Season 2   \n",
      "24  3754                         Tonikaku Kawaii 2nd Season   \n",
      "25  3745                                  Yuusha ga Shinda!   \n",
      "26  3742                Kono Subarashii Sekai ni Bakuen wo!   \n",
      "27  3738                                    Skip and Loafer   \n",
      "28  3696                        Kubo-san wa Mob o Yurusanai   \n",
      "\n",
      "                                                genre          status  score  \\\n",
      "0                 [CN Animation, Kiếm hiệp, Huyền ảo]  Đang tiến hành    5.8   \n",
      "1               [Anime, Hành động, Mecha, Viễn tưởng]      Hoàn thành    7.0   \n",
      "2                                      [CN Animation]  Đang tiến hành    8.7   \n",
      "3     [Anime, Hành động, Giả tưởng, Lịch sử, Kinh dị]      Hoàn thành    9.6   \n",
      "4   [Anime, Hành động, Hài hước, Giả tưởng, Học đư...      Hoàn thành    9.3   \n",
      "5   [Anime, Hành động, Giả tưởng, Học đường, Phiêu...      Hoàn thành    8.2   \n",
      "6                      [Anime, Hành động, Viễn tưởng]      Hoàn thành    3.1   \n",
      "7                  [Anime, Drama, Seinen, Siêu nhiên]      Hoàn thành    9.0   \n",
      "8                        [Anime, Đời thường, Âm nhạc]  Đang tiến hành    5.0   \n",
      "9                        [Anime, Tình cảm, Giả tưởng]      Hoàn thành    7.5   \n",
      "10                                   [Anime, Âm nhạc]      Hoàn thành    5.6   \n",
      "11              [Anime, Hài hước, Học đường, Shounen]      Hoàn thành    7.3   \n",
      "12              [Anime, Hành động, Giả tưởng, Shoujo]      Hoàn thành    4.5   \n",
      "13  [Anime, Hài hước, Tình cảm, Harem, Shounen, Ec...      Hoàn thành    8.7   \n",
      "14  [Anime, Hài hước, Tình cảm, Harem, Học đường, ...      Hoàn thành    6.7   \n",
      "15                [Anime, Hài hước, Tình cảm, Seinen]      Hoàn thành    8.5   \n",
      "16  [Anime, Hành động, Hài hước, Tình cảm, Harem, ...      Hoàn thành    7.5   \n",
      "17               [Anime, Hài hước, Giả tưởng, Seinen]      Hoàn thành    7.0   \n",
      "18                        [Anime, Tình cảm, Trò chơi]      Hoàn thành    8.9   \n",
      "19  [Anime, Hành động, Giả tưởng, Seinen, Phiêu lư...      Hoàn thành    8.1   \n",
      "20             [Anime, Hài hước, Shounen, Viễn tưởng]      Hoàn thành    5.6   \n",
      "21                       [Anime, Hài hước, Shoujo AI]      Hoàn thành    6.6   \n",
      "22            [Anime, Hành động, Shounen, Siêu nhiên]      Hoàn thành    7.7   \n",
      "23                                  [Anime, Thể thao]      Hoàn thành    6.4   \n",
      "24   [Anime, Hài hước, Tình cảm, Đời thường, Shounen]  Đang tiến hành    8.7   \n",
      "25  [Anime, Hành động, Hài hước, Harem, Giả tưởng,...      Hoàn thành    7.8   \n",
      "26                       [Anime, Hài hước, Giả tưởng]      Hoàn thành    8.9   \n",
      "27                          [Anime, Học đường, Drama]      Hoàn thành    8.2   \n",
      "28             [Anime, Hài hước, Tình cảm, Học đường]      Hoàn thành    9.2   \n",
      "\n",
      "    review  publish_year duration  \\\n",
      "0      122          2023   40 Tập   \n",
      "1       69          2023   12 Tập   \n",
      "2      316          2023   12 Tập   \n",
      "3      604          2023   13 Tập   \n",
      "4      608          2023   12 Tập   \n",
      "5      701          2023   13 Tập   \n",
      "6       55          2023   12 Tập   \n",
      "7      641          2023   11 Tập   \n",
      "8       54          2023   ?? Tập   \n",
      "9      125          2023   12 Tập   \n",
      "10      82          2023   12 Tập   \n",
      "11      94          2023   12 Tập   \n",
      "12      79          2023   12 Tập   \n",
      "13     229          2023   12 Tập   \n",
      "14      88          2023   12 Tập   \n",
      "15     196          2023   12 Tập   \n",
      "16     221          2023   12 Tập   \n",
      "17     141          2023   12 Tập   \n",
      "18     372          2023   13 Tập   \n",
      "19     256          2023   13 Tập   \n",
      "20      60          2023   12 Tập   \n",
      "21      98          2023   12 Tập   \n",
      "22     138          2023   12 Tập   \n",
      "23      74          2023   12 Tập   \n",
      "24     262          2023   12 Tập   \n",
      "25     176          2023   12 Tập   \n",
      "26     308          2023   12 Tập   \n",
      "27     133          2023   12 Tập   \n",
      "28     365          2023   12 Tập   \n",
      "\n",
      "                                                 link  \n",
      "0   https://animehay.in/thong-tin-phim/dan-dao-ton...  \n",
      "1   https://animehay.in/thong-tin-phim/mobile-suit...  \n",
      "2   https://animehay.in/thong-tin-phim/o-re-3781.html  \n",
      "3   https://animehay.in/thong-tin-phim/jigokuraku-...  \n",
      "4   https://animehay.in/thong-tin-phim/mashle-3752...  \n",
      "5   https://animehay.in/thong-tin-phim/isekai-de-c...  \n",
      "6   https://animehay.in/thong-tin-phim/the-margina...  \n",
      "7   https://animehay.in/thong-tin-phim/oshi-no-ko-...  \n",
      "8   https://animehay.in/thong-tin-phim/world-dai-s...  \n",
      "9   https://animehay.in/thong-tin-phim/kanojo-ga-k...  \n",
      "10  https://animehay.in/thong-tin-phim/kizuna-no-a...  \n",
      "11  https://animehay.in/thong-tin-phim/jijou-wo-sh...  \n",
      "12  https://animehay.in/thong-tin-phim/mahou-shouj...  \n",
      "13  https://animehay.in/thong-tin-phim/megami-no-c...  \n",
      "14  https://animehay.in/thong-tin-phim/rokudou-no-...  \n",
      "15  https://animehay.in/thong-tin-phim/otonari-ni-...  \n",
      "16  https://animehay.in/thong-tin-phim/isekai-shou...  \n",
      "17  https://animehay.in/thong-tin-phim/isekai-one-...  \n",
      "18  https://animehay.in/thong-tin-phim/yamada-kun-...  \n",
      "19  https://animehay.in/thong-tin-phim/tengoku-dai...  \n",
      "20  https://animehay.in/thong-tin-phim/kawaisugi-c...  \n",
      "21  https://animehay.in/thong-tin-phim/watashi-no-...  \n",
      "22  https://animehay.in/thong-tin-phim/edomae-elf-...  \n",
      "23  https://animehay.in/thong-tin-phim/birdie-wing...  \n",
      "24  https://animehay.in/thong-tin-phim/tonikaku-ka...  \n",
      "25  https://animehay.in/thong-tin-phim/yuusha-ga-s...  \n",
      "26  https://animehay.in/thong-tin-phim/kono-subara...  \n",
      "27  https://animehay.in/thong-tin-phim/skip-and-lo...  \n",
      "28  https://animehay.in/thong-tin-phim/kubo-san-wa...  \n"
     ]
    }
   ],
   "source": [
    "# Infinite scrapper\n",
    "proceed = True\n",
    "page = 14\n",
    "new_checkpoint_data = set()\n",
    "total_movie_data = []\n",
    "current_checkpoint_data = load_checkpoint()\n",
    "while(proceed):\n",
    "\n",
    "    print(page)\n",
    "    movie_items = get_movie_items(page)\n",
    "    for movie in movie_items:\n",
    "\n",
    "        checkpoint_data = get_checkpoint_data(movie)\n",
    "\n",
    "        # # Check if the movie item has ben processed on the previous batch or not\n",
    "        # if checkpoint_data in current_checkpoint_data:\n",
    "        #     # save_checkpoint(new_checkpoint_data)  # Save this batch as checkpoint for future scrapping batches\n",
    "        #     proceed = False  # Stop this movie item processing batch\n",
    "        #     break  \n",
    "\n",
    "        new_checkpoint_data.add(checkpoint_data)\n",
    "\n",
    "        movie_data = extract_data(movie)\n",
    "        if len(movie_data) == 0:  # Skip to next movie in case there is no data\n",
    "            continue\n",
    "\n",
    "        total_movie_data.append(convert_to_dict(id=checkpoint_data[0], data=movie_data))\n",
    "        print(movie_data)\n",
    "\n",
    "    page += 1\n",
    "    if page > 14:\n",
    "        proceed = False\n",
    "\n",
    "save_data(total_movie_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./data/checkpoint.pkl', 'wb') as f:\n",
    "    pickle.dump(set([(2713, '84/84')]), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(2713, '84/84')}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./data/checkpoint.pkl', 'rb') as f:\n",
    "    movie_data = pickle.load(f)\n",
    "\n",
    "movie_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrap -> check -> turn page or filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_crawling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
